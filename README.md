# lenaaltinger-Detecting-Annotation-Artifacts-for-NLI-Fine-Tuning-Pretrained-Transformers

This research outlines our work and outcomes in the LMU Munich Computational Linguistics master’s course on Biomedical Natural Language Processing. The seminar’s focus was on participating in the SemEval 2024 Task 2: Safe Biomedical Natural Language Inference, specifically addressing a textual entailment identification task. We investigate the performance of BERT-based models, including BERT-Base, BioMed-RoBERTa-base, BioBert-PubMed200kRCT and ClinicalBERT, in classifying Clinical Trial Records (CTRs) . Instead of participating in the proposed task, our focus lies on detecting potential annotation artifacts, where models may exploit patterns in the data without a genuine understanding of the task. We explore various model architectures and training input configurations, revealing that models can achieve reasonable performance even when lacking essential information. Our findings underscore the prevalence of annotation bias and raise important considerations for refining task formulations and enhancing model generalization in biomedical natural language processing tasks.

The code is seperated in sections and commented, which is why no further explanations are given here.
